{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ZheniNikolaevaDimitr\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import os \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainig_list = glob.glob(os.path.join(os.getcwd(), \"./data/train-mails\", \"*.txt\"))\n",
    "test_data = glob.glob(os.path.join(os.getcwd(), \"./data/test-mails\", \"*.txt\")) #with those 2 we get the training and testing data from the folders\n",
    "\n",
    "training_data =  pd.DataFrame(columns=['text', 'spam'])\n",
    "test_data =  pd.DataFrame(columns=['text', 'spam']) #create dataframe - creating columns text and spam\n",
    "\n",
    "for index, file_path in enumerate(trainig_list):\n",
    "    with open(file_path) as f_input:\n",
    "        spam = 0\n",
    "        if 'spmsg' in file_path:\n",
    "            spam = 1\n",
    "        training_data.loc[index] = [f_input.read(), spam] #parse the data from the training list and see whether its a spam or not based on spmmsg tag in title\n",
    "    \n",
    "for file_path in trainig_list:\n",
    "    with open(file_path) as f_input:\n",
    "        spam = 0\n",
    "        if 'spmsg' in file_path:\n",
    "            spam = 1\n",
    "        test_data.loc[index] = [f_input.read(), spam] #does the same? ask toms\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: re : 2 . 882 s - &gt; np np\\n\\n&gt; deat : ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: s - &gt; np + np\\n\\ndiscussion s - &gt; np ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: 2 . 882 s - &gt; np np\\n\\n. . . 's much ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text spam\n",
       "0  Subject: re : 2 . 882 s - > np np\\n\\n> deat : ...    0\n",
       "1  Subject: s - > np + np\\n\\ndiscussion s - > np ...    0\n",
       "2  Subject: 2 . 882 s - > np np\\n\\n. . . 's much ...    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape #it will give the answer , show data as, as rows and columns , it makes it like that by default, with this you can see how the data looks like\n",
    "training_data.head(3) # if i put 5 it will show 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      subject 2 882 np np deat sun 15 dec 91 2 25 2 ...\n",
       "1      subject np np discuss np np remind ago read so...\n",
       "2      subject 2 882 np np much restrict np np np pro...\n",
       "3      subject gent confer listserv intern confer 199...\n",
       "4      subject queri caus korean anyon point book art...\n",
       "5      subject l2 learn cultur empathi graduat studen...\n",
       "6      subject psycholinguist teach undergradu cours ...\n",
       "7      subject german corpora line corpu modern germa...\n",
       "8      subject hi help design experi mandarin tone pa...\n",
       "9      subject job univers utah linguist program univ...\n",
       "10     subject job montreal post de professeur e regu...\n",
       "11     subject lectureship linguist c h l f e n g l h...\n",
       "12     subject neg concord interest grammar neg conco...\n",
       "13     subject neg concord tri compil list commerci a...\n",
       "14     subject queri email address doe anybodi email ...\n",
       "15     subject post hi work phonet project modern iri...\n",
       "16     subject queri iceland anyon tell anyth whether...\n",
       "17     subject search doe anyon whereabout dr peter h...\n",
       "18     subject 3 378 korean l2 psycholinguist german ...\n",
       "19     subject 3 370 queri comput resourc dialect map...\n",
       "20     subject languag power bibliographi announc bib...\n",
       "21     subject 3 376 help iaup hay alguien que sepa c...\n",
       "22     subject next messag attempt linguist commun ne...\n",
       "23     subject queri english interrog appreci refer d...\n",
       "24     subject queri spanish bind request help form r...\n",
       "25     subject proofread mediev e text humanist discu...\n",
       "26     subject learn french quebec citi de ami de l u...\n",
       "27     subject human subject number month ago rais is...\n",
       "28     subject rule guido vanden wyngaerd 3 357 must ...\n",
       "29     subject tone grammar summari final lans point ...\n",
       "                             ...                        \n",
       "672    subject 55 550 hard work n pass select receiv ...\n",
       "673    subject free hot adult site password select re...\n",
       "674    subject person messag place ad one small area ...\n",
       "675    subject scu fi convent hi ask pass interest la...\n",
       "676    subject bulletproof web host 50 per month fres...\n",
       "677    subject hello realli blast movi virtual realit...\n",
       "678    subject fanci flutter tip 20 tipp winner week ...\n",
       "679    subject debt fast ukjx dear fellow entrepreneu...\n",
       "680    subject lifeplan lifeplan goal set sharewar li...\n",
       "681    subject conquer thin hair dandruff problem for...\n",
       "682    subject stop pay high major airlin fare stop p...\n",
       "683    subject bull one messag compos extractor pro b...\n",
       "684    subject sampl adult adult five minut free toda...\n",
       "685    subject total profit 980 1998 1998 stock pick ...\n",
       "686    subject incred opportun dear friend receiv mes...\n",
       "687    subject inform http www privategold com cgi bi...\n",
       "688    subject possibl 900 stock invest return stock ...\n",
       "689    subject releas cd vol 4a onlin busi got frustr...\n",
       "690    subject latest greatest cd vol 4a onlin busi g...\n",
       "691    subject prove wealth build system 9113 innov p...\n",
       "692    subject press releas 6 28 99 cd vol 5 onlin bu...\n",
       "693    subject judici judgment child support investig...\n",
       "694    subject publish compani sale inform free credi...\n",
       "695    subject suit commut greet opportun lifetim kno...\n",
       "696    subject home base travel biz free cruis 10k 1s...\n",
       "697    subject vote 1 onlin busi hi drop quick note l...\n",
       "698    subject read dream member team earn 6 figur in...\n",
       "699    subject search credibl dream member team earn ...\n",
       "700    subject financi freedom sleep dear achiev inte...\n",
       "701    subject major acquisit compani mark industri s...\n",
       "Name: text, Length: 702, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML markup\n",
    "    text = re.sub('<[^/>]*>', ' ', text)\n",
    "    # remove no-words\n",
    "    text = re.sub('[\\W]', ' ', text)\n",
    "    # convert all to lower case\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    porter = PorterStemmer() #to get rid of the multiple similar words, get the base of the word == run running ran / for that it will give run\n",
    "    text = [porter.stem(word) for word in text if not word in stop]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text; #this method cleans the data from prepinatelni znaci and useless words = and or the\n",
    "\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(preprocessor) # apply the above method on the training data\n",
    "training_data['text']#displaying the result of what we did up there in this cell, down we have a display of the cleaned training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words # with this method we create a dictionary, which is a matrix that will show how many times each word was repeated in one email\n",
    "    \n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list(list_to_remove):\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary # we make the list to remove the words that are repeated once or short words = look up the tutirial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir): \n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000)) # we make another matrix with 3000 columns / see on tutorial / to extract the features from the directory\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi): #looping throug the lines of the file ; enumerate might meand turn text datat into nummeric\n",
    "          if i == 2: #skiping first 2 lines of email cause its cc and empty line\n",
    "            words = line.split() # creating array of words from a line of the file \n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary): \n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1     \n",
    "    return features_matrix # this matrix has word and emails / it is a representation of how many times a selected word has beeen used in each of the provided mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** List of imports for classifiers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re #importing studff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a dictionary of words with its frequency **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './data/train-mails' #we target the training directory\n",
    "dictionary = make_Dictionary(train_dir)  # we creati a dictionary from the taining directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prepare feature vectors per training mail and its labels **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(702)# creates an empty array or dataset\n",
    "train_labels[351:701] = 1 \n",
    "train_matrix = extract_features(train_dir) #creates training  matrix using extract features function\n",
    "print(train_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training SVM and Naive bayes classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=1, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyNB = MultinomialNB()\n",
    "MySVC = LinearSVC()\n",
    "MySGDC = SGDClassifier(loss='log', random_state=1, max_iter=1) #first 3 lines is creating the classifiers\n",
    "MyNB.fit(train_matrix,train_labels)\n",
    "MySVC.fit(train_matrix,train_labels)\n",
    "MySGDC.fit(train_matrix,train_labels)#last 3 lines is training the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test the unseen mails for Spam **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MultinomialNB: Accuracy is ', 96.15384615384616)\n",
      "('SVC: Accuracy is ', 96.15384615384616)\n",
      "('SGDC: Accuracy is ', 93.07692307692308)\n"
     ]
    }
   ],
   "source": [
    "test_dir = './data/test-mails' #set test directory\n",
    "test_matrix = extract_features(test_dir) #extract features using the method from above in order to test the data\n",
    "test_labels = np.zeros(260) #creating an empty array with 260 elements\n",
    "test_labels[130:260] = 1 \n",
    "\n",
    "\n",
    "result_MyNB = MyNB.predict(test_matrix)\n",
    "print (\"MultinomialNB: Accuracy is \", accuracy_score(test_labels, result_MyNB)*100)\n",
    "\n",
    "result_MySVC = MySVC.predict(test_matrix)\n",
    "print (\"SVC: Accuracy is \", accuracy_score(test_labels, result_MySVC)*100)\n",
    "\n",
    "result_MySGDC = MySGDC.predict(test_matrix)\n",
    "print (\"SGDC: Accuracy is \", accuracy_score(test_labels, result_MySGDC)*100) # create predictions and print them to see which algorithm is the most accurate\n",
    "\n",
    "# Create the confusion matrix fot Mayes and SVC\n",
    "confusion_mat_NB = confusion_matrix(test_labels,result_MyNB)\n",
    "confusion_mat_SVC = confusion_matrix(test_labels,result_MySVC)\n",
    "confusion_mat_SGDC = confusion_matrix(test_labels,result_MySGDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Visualize confusion matrix **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0c4b73e2a06c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'True labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m#we have true and predicted labels  ,read on her slides\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#thw numbers will be displayed underneath and are showing numbers corresponding to the color scale and based on those numbers tha colots are being computed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\matplotlib\\pyplot.pyc\u001b[0m in \u001b[0;36mshow\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \"\"\"\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.pyc\u001b[0m in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     37\u001b[0m             display(\n\u001b[0;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             )\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.pyc\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;31m# determine if a background is needed for legibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;31m# the background is transparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.pyc\u001b[0m in \u001b[0;36m_is_transparent\u001b[1;34m(color)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "# Visualize confusion matrix for bayes\n",
    "plt.imshow(confusion_mat_NB, interpolation='nearest')\n",
    "plt.title('Confusion matrix: MultinomialNB')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(2)\n",
    "plt.xticks(ticks, ticks)\n",
    "plt.yticks(ticks, ticks)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show() \n",
    "#we have true and predicted labels  ,read on her slides \n",
    "#thw numbers will be displayed underneath and are showing numbers corresponding to the color scale and based on those numbers tha colots are being computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix for SVC\n",
    "plt.imshow(confusion_mat_SVC, interpolation='nearest')\n",
    "plt.title('Confusion matrix: SVC')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(2)\n",
    "plt.xticks(ticks, ticks)\n",
    "plt.yticks(ticks, ticks)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix for SGDC\n",
    "plt.imshow(confusion_mat_SGDC, interpolation='nearest')\n",
    "plt.title('Confusion matrix: SGDC')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(2)\n",
    "plt.xticks(ticks, ticks)\n",
    "plt.yticks(ticks, ticks)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MultinomialNB: \")\n",
    "print(confusion_matrix(result_MyNB, test_labels))\n",
    "print(\"SVC: \")\n",
    "print(confusion_matrix(result_MySVC, test_labels))\n",
    "print(\"SGDC: \")\n",
    "print(confusion_matrix(result_MySGDC, test_labels))\n",
    "\n",
    "# here we get the numbers for each confusion matrix that are determinig the color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prediction for Bayes **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions for Bayes\n",
    "predictionsNB = MyNB.predict(test_matrix)\n",
    "print('Accuracy score: ', format(accuracy_score(test_labels, predictionsNB)))\n",
    "print('Precision score: ', format(precision_score(test_labels, predictionsNB)))\n",
    "print('Recall score: ', format(recall_score(test_labels, predictionsNB)))\n",
    "print('F1 score: ', format(f1_score(test_labels, predictionsNB))) # printing all these out to validate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "predictionsSVC = MySVC.predict(test_matrix)\n",
    "print('Accuracy score: ', format(accuracy_score(test_labels, predictionsSVC)))\n",
    "print('Precision score: ', format(precision_score(test_labels, predictionsSVC)))\n",
    "print('Recall score: ', format(recall_score(test_labels, predictionsSVC)))\n",
    "print('F1 score: ', format(f1_score(test_labels, predictionsSVC))) # printing all these out to validate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "predictionsSGDC = MySGDC.predict(test_matrix)\n",
    "print('Accuracy score: ', format(accuracy_score(test_labels, predictionsSGDC)))\n",
    "print('Precision score: ', format(precision_score(test_labels, predictionsSGDC)))\n",
    "print('Recall score: ', format(recall_score(test_labels, predictionsSGDC)))\n",
    "print('F1 score: ', format(f1_score(test_labels, predictionsSGDC))) # printing all these out to validate the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir and subdir for pickled objects (export of the built model)\n",
    "dest = os.path.join('model', 'pickles')# create a directory where to save our model\n",
    "if not os.path.exists(dest):#if it doesnt exist\n",
    "    os.makedirs(dest)#create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(MyNB, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
    "# we serialize it in order to save it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd() # get current directory\n",
    "stop = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'stopwords.pkl'), 'rb')) #we load the stopwords \n",
    "\n",
    "\n",
    "\n",
    "classifier = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'classifier.pkl'), 'rb'))    # we load the classifier   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer) # it is tokanizing stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse the model in order ot make a new prediction\n",
    "label = {0:'ham', 1:'spam'} # 0 is ham and 1 is spam, ham means teh mail is not spam\n",
    "\n",
    "# predict() returns predicted label\n",
    "# predict_proba(X) returns probability\n",
    "example_dir = './data/example' #we make a new directory with only 1 file\n",
    "X = extract_features(example_dir)#we extract the features in that X directory in order to make a prediction based on the features\n",
    "\n",
    "print('Prediction 1: %s\\nProbability 1: %.2f%%' %(label[classifier.predict(X)[0]], np.max(classifier.predict_proba(X))*100))\n",
    "#print(np.max(classifier.predict_proba(X)))\n",
    "\n",
    "\n",
    "#example2 = ['Last chance to save! We are extending our Cyber Monday Sale through the end of today. You wonâ€™t see another deal like this until next year, so act now for 30% off annual team accounts.']\n",
    "#X = vect.transform(example2)\n",
    "#print('Prediction 2: %s\\nProbability 2: %.2f%%' %(label[classifier.predict(X)[0]], np.max(classifier.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop the Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mywebapp.py\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os #imports\n",
    "from flask import Flask, request, session, g, redirect, url_for, abort, \\\n",
    "     render_template, flash, jsonify\n",
    "app = Flask(__name__, template_folder='./templates') #initialize the flask - used to create web apps with python- and we set the templates foldet for the template of the html\n",
    "\n",
    "label = {0:'ham', 1:'spam'}\n",
    "stop = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'stopwords.pkl'), 'rb')) # loading the stopwords\n",
    "\n",
    "classifier = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'classifier.pkl'), 'rb')) # loading the classifier\n",
    "\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words \n",
    "    \n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list(list_to_remove):\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary #same function to create a dictionary - we put it here in order for pythn to be able to use it \n",
    "\n",
    "\n",
    "def extract_features(mail_dir): \n",
    "    train_dir = './data/train-mails'\n",
    "    dictionary = make_Dictionary(train_dir)\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1     \n",
    "    return features_matrix # same extract feature because this is a python file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "  return render_template('index.html') # set the route\n",
    "\n",
    "def write_mail_to_file(mailContent = ''):\n",
    "  path = './data/example'\n",
    "  filename = 'example.txt'\n",
    "  if not os.path.exists(path):\n",
    "      os.makedirs(path) \n",
    "\n",
    "  f = open(os.path.join(path, filename), 'w+')\n",
    "  f.write(mailContent)\n",
    "  f.close() #function tp save the mail tht we he have inputted to save it in the file exapmle . txt in the folder data/example\n",
    "\n",
    "@app.route(\"/check-email\", methods=['POST'])\n",
    "def check_mail():\n",
    "  try:\n",
    "      print(request.form)\n",
    "      write_mail_to_file(request.form['mailContent'])\n",
    "      example_dir = './data/example'\n",
    "      X = extract_features(example_dir)\n",
    "\n",
    "      print('Prediction 1: %s\\nProbability 1: %.2f%%' %(label[classifier.predict(X)[0]], np.max(classifier.predict_proba(X))*100))\n",
    "\n",
    "      res = {\"success\": True, \"isSpam\": label[classifier.predict(X)[0]], \"probability\": np.max(classifier.predict_proba(X))*100}\n",
    "      return jsonify(res)\n",
    "  except Exception as e:\n",
    "        print(e)\n",
    "        res = {\"success\": False, \"error\": e}\n",
    "        return jsonify(res)  # post method to make the request based on what you have and return result wether its a spam or not and return the probabilit of it being true\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  app.run(port = 5002, debug=True) # this is for flask so it knows on which port python flask app should run the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess as sub\n",
    "\n",
    "# this opens a windows terminal\n",
    "sub.call('start /wait python mywebapp.py', shell=True) #run the app and open the terminal sp we can see what errors we get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
