{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ZheniNikolaevaDimitr\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainig_list = glob.glob(os.path.join(os.getcwd(), \"./data/train-mails\", \"*.txt\"))\n",
    "test_data = glob.glob(os.path.join(os.getcwd(), \"./data/test-mails\", \"*.txt\"))\n",
    "\n",
    "training_data =  pd.DataFrame(columns=['text', 'spam'])\n",
    "test_data =  pd.DataFrame(columns=['text', 'spam'])\n",
    "\n",
    "for index, file_path in enumerate(trainig_list):\n",
    "    with open(file_path) as f_input:\n",
    "        spam = 0\n",
    "        if 'spmsg' in file_path:\n",
    "            spam = 1\n",
    "        training_data.loc[index] = [f_input.read(), spam]\n",
    "    \n",
    "for file_path in trainig_list:\n",
    "    with open(file_path) as f_input:\n",
    "        spam = 0\n",
    "        if 'spmsg' in file_path:\n",
    "            spam = 1\n",
    "        test_data.loc[index] = [f_input.read(), spam]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: re : 2 . 882 s - &gt; np np\\n\\n&gt; deat : ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: s - &gt; np + np\\n\\ndiscussion s - &gt; np ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: 2 . 882 s - &gt; np np\\n\\n. . . 's much ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text spam\n",
       "0  Subject: re : 2 . 882 s - > np np\\n\\n> deat : ...    0\n",
       "1  Subject: s - > np + np\\n\\ndiscussion s - > np ...    0\n",
       "2  Subject: 2 . 882 s - > np np\\n\\n. . . 's much ...    0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape\n",
    "training_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      subject 2 882 np np deat sun 15 dec 91 2 25 2 ...\n",
       "1      subject np np discuss np np remind ago read so...\n",
       "2      subject 2 882 np np much restrict np np np pro...\n",
       "3      subject gent confer listserv intern confer 199...\n",
       "4      subject queri caus korean anyon point book art...\n",
       "5      subject l2 learn cultur empathi graduat studen...\n",
       "6      subject psycholinguist teach undergradu cours ...\n",
       "7      subject german corpora line corpu modern germa...\n",
       "8      subject hi help design experi mandarin tone pa...\n",
       "9      subject job univers utah linguist program univ...\n",
       "10     subject job montreal post de professeur e regu...\n",
       "11     subject lectureship linguist c h l f e n g l h...\n",
       "12     subject neg concord interest grammar neg conco...\n",
       "13     subject neg concord tri compil list commerci a...\n",
       "14     subject queri email address doe anybodi email ...\n",
       "15     subject post hi work phonet project modern iri...\n",
       "16     subject queri iceland anyon tell anyth whether...\n",
       "17     subject search doe anyon whereabout dr peter h...\n",
       "18     subject 3 378 korean l2 psycholinguist german ...\n",
       "19     subject 3 370 queri comput resourc dialect map...\n",
       "20     subject languag power bibliographi announc bib...\n",
       "21     subject 3 376 help iaup hay alguien que sepa c...\n",
       "22     subject next messag attempt linguist commun ne...\n",
       "23     subject queri english interrog appreci refer d...\n",
       "24     subject queri spanish bind request help form r...\n",
       "25     subject proofread mediev e text humanist discu...\n",
       "26     subject learn french quebec citi de ami de l u...\n",
       "27     subject human subject number month ago rais is...\n",
       "28     subject rule guido vanden wyngaerd 3 357 must ...\n",
       "29     subject tone grammar summari final lans point ...\n",
       "                             ...                        \n",
       "672    subject 55 550 hard work n pass select receiv ...\n",
       "673    subject free hot adult site password select re...\n",
       "674    subject person messag place ad one small area ...\n",
       "675    subject scu fi convent hi ask pass interest la...\n",
       "676    subject bulletproof web host 50 per month fres...\n",
       "677    subject hello realli blast movi virtual realit...\n",
       "678    subject fanci flutter tip 20 tipp winner week ...\n",
       "679    subject debt fast ukjx dear fellow entrepreneu...\n",
       "680    subject lifeplan lifeplan goal set sharewar li...\n",
       "681    subject conquer thin hair dandruff problem for...\n",
       "682    subject stop pay high major airlin fare stop p...\n",
       "683    subject bull one messag compos extractor pro b...\n",
       "684    subject sampl adult adult five minut free toda...\n",
       "685    subject total profit 980 1998 1998 stock pick ...\n",
       "686    subject incred opportun dear friend receiv mes...\n",
       "687    subject inform http www privategold com cgi bi...\n",
       "688    subject possibl 900 stock invest return stock ...\n",
       "689    subject releas cd vol 4a onlin busi got frustr...\n",
       "690    subject latest greatest cd vol 4a onlin busi g...\n",
       "691    subject prove wealth build system 9113 innov p...\n",
       "692    subject press releas 6 28 99 cd vol 5 onlin bu...\n",
       "693    subject judici judgment child support investig...\n",
       "694    subject publish compani sale inform free credi...\n",
       "695    subject suit commut greet opportun lifetim kno...\n",
       "696    subject home base travel biz free cruis 10k 1s...\n",
       "697    subject vote 1 onlin busi hi drop quick note l...\n",
       "698    subject read dream member team earn 6 figur in...\n",
       "699    subject search credibl dream member team earn ...\n",
       "700    subject financi freedom sleep dear achiev inte...\n",
       "701    subject major acquisit compani mark industri s...\n",
       "Name: text, Length: 702, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML markup\n",
    "    text = re.sub('<[^/>]*>', ' ', text)\n",
    "    # remove no-words\n",
    "    text = re.sub('[\\W]', ' ', text)\n",
    "    # convert all to lower case\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    text = [porter.stem(word) for word in text if not word in stop]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text;\n",
    "\n",
    "\n",
    "training_data['text'] = training_data['text'].apply(preprocessor)\n",
    "training_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    \n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list(list_to_remove):\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir): \n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1     \n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** List of imports for classifiers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a dictionary of words with its frequency **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './data/train-mails'\n",
    "dictionary = make_Dictionary(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prepare feature vectors per training mail and its labels **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(702)\n",
    "train_labels[351:701] = 1\n",
    "train_matrix = extract_features(train_dir)\n",
    "print(train_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training SVM and Naive bayes classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=1, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyNB = MultinomialNB()\n",
    "MySVC = LinearSVC()\n",
    "MySGDC = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "MyNB.fit(train_matrix,train_labels)\n",
    "MySVC.fit(train_matrix,train_labels)\n",
    "MySGDC.fit(train_matrix,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test the unseen mails for Spam **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = './data/test-mails'\n",
    "test_matrix = extract_features(test_dir)\n",
    "test_labels = np.zeros(260)\n",
    "test_labels[130:260] = 1\n",
    "\n",
    "\n",
    "result_MyNB = MyNB.predict(test_matrix)\n",
    "print (\"MultinomialNB: Accuracy is \", accuracy_score(test_labels, result_MyNB)*100)\n",
    "\n",
    "result_MySVC = MySVC.predict(test_matrix)\n",
    "print (\"SVC: Accuracy is \", accuracy_score(test_labels, result_MySVC)*100)\n",
    "\n",
    "result_MySGDC = MySGDC.predict(test_matrix)\n",
    "print (\"SGDC: Accuracy is \", accuracy_score(test_labels, result_MySGDC)*100)\n",
    "\n",
    "# Create the confusion matrix fot Mayes and SVC\n",
    "confusion_mat_NB = confusion_matrix(test_labels,result_MyNB)\n",
    "confusion_mat_SVC = confusion_matrix(test_labels,result_MySVC)\n",
    "confusion_mat_SGDC = confusion_matrix(test_labels,result_MySGDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Visualize confusion matrix **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-97af2095fa60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'True labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \"\"\"\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.pyc\u001b[0m in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     37\u001b[0m             display(\n\u001b[0;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             )\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.pyc\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;31m# determine if a background is needed for legibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;31m# the background is transparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[1;32mC:\\Users\\ZheniNikolaevaDimitr\\Anaconda2\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.pyc\u001b[0m in \u001b[0;36m_is_transparent\u001b[1;34m(color)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "# Visualize confusion matrix for bayes\n",
    "plt.imshow(confusion_mat_NB, interpolation='nearest')\n",
    "plt.title('Confusion matrix: MultinomialNB')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(2)\n",
    "plt.xticks(ticks, ticks)\n",
    "plt.yticks(ticks, ticks)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix for SVC\n",
    "plt.imshow(confusion_mat_SVC, interpolation='nearest')\n",
    "plt.title('Confusion matrix: SVC')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(2)\n",
    "plt.xticks(ticks, ticks)\n",
    "plt.yticks(ticks, ticks)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix for SGDC\n",
    "plt.imshow(confusion_mat_SGDC, interpolation='nearest')\n",
    "plt.title('Confusion matrix: SGDC')\n",
    "plt.colorbar()\n",
    "ticks = np.arange(2)\n",
    "plt.xticks(ticks, ticks)\n",
    "plt.yticks(ticks, ticks)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MultinomialNB: \")\n",
    "print(confusion_matrix(result_MyNB, test_labels))\n",
    "print(\"SVC: \")\n",
    "print(confusion_matrix(result_MySVC, test_labels))\n",
    "print(\"SGDC: \")\n",
    "print(confusion_matrix(result_MySGDC, test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prediction for Bayes **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions for Bayes\n",
    "predictionsNB = MyNB.predict(test_matrix)\n",
    "print('Accuracy score: ', format(accuracy_score(test_labels, predictionsNB)))\n",
    "print('Precision score: ', format(precision_score(test_labels, predictionsNB)))\n",
    "print('Recall score: ', format(recall_score(test_labels, predictionsNB)))\n",
    "print('F1 score: ', format(f1_score(test_labels, predictionsNB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "predictionsSVC = MySVC.predict(test_matrix)\n",
    "print('Accuracy score: ', format(accuracy_score(test_labels, predictionsSVC)))\n",
    "print('Precision score: ', format(precision_score(test_labels, predictionsSVC)))\n",
    "print('Recall score: ', format(recall_score(test_labels, predictionsSVC)))\n",
    "print('F1 score: ', format(f1_score(test_labels, predictionsSVC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "predictionsSGDC = MySGDC.predict(test_matrix)\n",
    "print('Accuracy score: ', format(accuracy_score(test_labels, predictionsSGDC)))\n",
    "print('Precision score: ', format(precision_score(test_labels, predictionsSGDC)))\n",
    "print('Recall score: ', format(recall_score(test_labels, predictionsSGDC)))\n",
    "print('F1 score: ', format(f1_score(test_labels, predictionsSGDC)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir and subdir for pickled objects (export of the built model)\n",
    "dest = os.path.join('model', 'pickles')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(MyNB, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "stop = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "classifier = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'classifier.pkl'), 'rb'))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse the model for prediction\n",
    "label = {0:'ham', 1:'spam'}\n",
    "\n",
    "# predict() returns predicted label\n",
    "# predict_proba(X) returns probability\n",
    "example_dir = './data/example'\n",
    "X = extract_features(example_dir)\n",
    "\n",
    "print('Prediction 1: %s\\nProbability 1: %.2f%%' %(label[classifier.predict(X)[0]], np.max(classifier.predict_proba(X))*100))\n",
    "#print(np.max(classifier.predict_proba(X)))\n",
    "\n",
    "\n",
    "#example2 = ['Last chance to save! We are extending our Cyber Monday Sale through the end of today. You wonâ€™t see another deal like this until next year, so act now for 30% off annual team accounts.']\n",
    "#X = vect.transform(example2)\n",
    "#print('Prediction 2: %s\\nProbability 2: %.2f%%' %(label[classifier.predict(X)[0]], np.max(classifier.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop the Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mywebapp.py\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from flask import Flask, request, session, g, redirect, url_for, abort, \\\n",
    "     render_template, flash, jsonify\n",
    "app = Flask(__name__, template_folder='./templates')\n",
    "\n",
    "label = {0:'ham', 1:'spam'}\n",
    "stop = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "classifier = pickle.load(open(\n",
    "                os.path.join('model', \n",
    "                'pickles', \n",
    "                'classifier.pkl'), 'rb'))\n",
    "\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    \n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = dictionary.keys()\n",
    "    for item in list(list_to_remove):\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def extract_features(mail_dir): \n",
    "    train_dir = './data/train-mails'\n",
    "    dictionary = make_Dictionary(train_dir)\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1     \n",
    "    return features_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "  return render_template('index.html')\n",
    "\n",
    "def write_mail_to_file(mailContent = ''):\n",
    "  path = './data/example'\n",
    "  filename = 'example.txt'\n",
    "  if not os.path.exists(path):\n",
    "      os.makedirs(path)\n",
    "\n",
    "  f = open(os.path.join(path, filename), 'w+')\n",
    "  f.write(mailContent)\n",
    "  f.close()\n",
    "\n",
    "@app.route(\"/check-email\", methods=['POST'])\n",
    "def check_mail():\n",
    "  try:\n",
    "      print(request.form)\n",
    "      write_mail_to_file(request.form['mailContent'])\n",
    "      example_dir = './data/example'\n",
    "      X = extract_features(example_dir)\n",
    "\n",
    "      print('Prediction 1: %s\\nProbability 1: %.2f%%' %(label[classifier.predict(X)[0]], np.max(classifier.predict_proba(X))*100))\n",
    "\n",
    "      res = {\"success\": True, \"isSpam\": label[classifier.predict(X)[0]], \"probability\": np.max(classifier.predict_proba(X))*100}\n",
    "      return jsonify(res)\n",
    "  except Exception as e:\n",
    "        print(e)\n",
    "        res = {\"success\": False, \"error\": e}\n",
    "        return jsonify(res)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  app.run(port = 5002, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess as sub\n",
    "\n",
    "# this opens a windows terminal\n",
    "sub.call('start /wait python mywebapp.py', shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
